{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим обработанные данные обзоров и соответсвующих обзору оценок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>служба доставка оставаться крайне негативный в...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>недобросовестный транспортный компания</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>хорошо почта россия</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ужасный контора</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>большой срок доставка нарушение срок посылка у...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0  служба доставка оставаться крайне негативный в...          0\n",
       "1             недобросовестный транспортный компания          0\n",
       "2                                хорошо почта россия          0\n",
       "3                                    ужасный контора          0\n",
       "4  большой срок доставка нарушение срок посылка у...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('Clean_goods.csv')\n",
    "df=df.dropna().reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7281"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Будем работать с list of tuple обзоров-тонов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "служба доставка оставаться крайне негативный впечатление\n"
     ]
    }
   ],
   "source": [
    "print df['Review'].iloc[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews=zip(df['Review'].iloc[:],df['Sentiment'].iloc[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посмотрим сколько положительных и отрицательных отзывов в нашей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BadGoodReview(Reviews_Sentiments):\n",
    "    bad=0\n",
    "    good=0\n",
    "    for item in Reviews_Sentiments:\n",
    "        if int(item[1])==1:\n",
    "            good=good+1\n",
    "        elif int(item[1])==0:\n",
    "            bad=bad+1\n",
    "    return (good,bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good bad\n",
      "(3076, 4205)\n"
     ]
    }
   ],
   "source": [
    "print 'good','bad'\n",
    "print BadGoodReview(clean_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## сделаем выборку 50 на 50 для удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fifty(reviews_sentiments):\n",
    "    result=[]\n",
    "    num_of_min=min(BadGoodReview(reviews_sentiments))\n",
    "    for review in clean_reviews:\n",
    "        if int(review[1])==1:\n",
    "            result.append((review[0],review[1]))\n",
    "        elif int(review[1])==0 and num_of_min>0:\n",
    "            result.append((review[0],review[1]))\n",
    "            num_of_min=num_of_min-1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews=fifty(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good bad\n",
      "(3076, 3076)\n"
     ]
    }
   ],
   "source": [
    "print 'good','bad'\n",
    "print BadGoodReview(clean_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучим модели на разных классификаторах, с функциями взвешивания tf-idf и count vectorizer, с разными комбинациями n-грамм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('N-gram Scheme:', (1, 1))\n",
      "Count Vectorizer\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9f3139e82c51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclean_reviews\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentiment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclean_reviews\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mShuffleSplit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# массив n-граммных схем, которые будут использоваться в работе\n",
    "# например, (1, 3) означает униграммы + биграммы + триграммы\n",
    "ngram_schemes = [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6)]\n",
    "\n",
    "for ngram_scheme in ngram_schemes:\n",
    "\n",
    "    print('N-gram Scheme:', ngram_scheme)\n",
    "\n",
    "    count_vectorizer = CountVectorizer(analyzer = \"word\",ngram_range=ngram_scheme,tokenizer = None, preprocessor = None, stop_words = None,max_features = 3000)\n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer = \"word\", ngram_range=ngram_scheme,max_features=3000)\n",
    "\n",
    "    vectorizers = [count_vectorizer, tfidf_vectorizer]\n",
    "    vectorizers_names = ['Count Vectorizer', 'TF-IDF Vectorizer']\n",
    "\n",
    "    for i in range(len(vectorizers)):\n",
    "        print(vectorizers_names[i])\n",
    "        vectorizer = vectorizers[i]\n",
    "\n",
    "        X = vectorizer.fit_transform([sentence[0] for sentence in clean_reviews])\n",
    "        y = [int(sentiment[1]) for sentiment in clean_reviews]\n",
    "        np.asarray(X)\n",
    "        \n",
    "        cv = ShuffleSplit( n_splits=5, test_size=0.2, random_state=0)\n",
    "        \n",
    "        classifiers=[]\n",
    "        classifiers.append(('RF', RandomForestClassifier(n_estimators = 100)))\n",
    "        classifiers.append(('AB', AdaBoostClassifier()))\n",
    "        classifiers.append(('DT', DecisionTreeClassifier()))\n",
    "        classifiers.append(('LR', LogisticRegression(solver = 'lbfgs', max_iter = 500, multi_class = 'auto')))\n",
    "        classifiers.append(('NB', GaussianNB()))\n",
    "        classifiers.append(('SVR', SVC(gamma = 2, C = 1.0))) # gamma - коэффициент ядра для 'rbf' - radial basis function, 'poly' and 'sigmoid'\n",
    "        classifiers.append(('MLP', MLPClassifier(alpha = 0.01, max_iter = 200, solver = 'lbfgs', tol = 0.001)))\n",
    "        classifiers.append(('KNN', KNeighborsClassifier(2)))\n",
    "        \n",
    "        for clf in classifiers:\n",
    "            print clf[0], ':'\n",
    "            print cross_val_score(clf[1],X.toarray(),y,cv=cv).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как видно лучший результат показывают униграммы + tf-idf мера+ SVR классификатор. Будем работать с ним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## перемешаем обзоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=np.split(clean_reviews, [int(.8*len(clean_reviews))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_scheme = (1, 1)\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\", ngram_range=ngram_scheme,max_features=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = vectorizer.fit_transform([sentence[0] for sentence in train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<4921x3000 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 17874 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(train_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## тестовая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<1231x3000 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 4126 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_features = vectorizer.transform([sentence[0]for sentence in test])\n",
    "np.asarray(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8870836718115354\n",
      "precision 0.8869426751592356\n",
      "recall 0.8912\n",
      "f1 measure 0.8890662410215483\n",
      "Wall time: 6.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf= SVC(gamma = 1.0, C = 1.5, probability=True)\n",
    "clf=clf.fit(train_data_features, [int(sentiment[1]) for sentiment in train])\n",
    "\n",
    "\n",
    "result= clf.predict(test_data_features)\n",
    "\n",
    "print'accuracy', metrics.accuracy_score([int(sentiment[1]) for sentiment in test],result)\n",
    "print 'precision', metrics.precision_score([int(sentiment[1]) for sentiment in test],result)\n",
    "print'recall', metrics.recall_score([int(sentiment[1]) for sentiment in test],result)\n",
    "print'f1 measure', metrics.f1_score([int(sentiment[1]) for sentiment in test],result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96209551 0.03790449]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print clf.predict_proba(test_data_features[0])\n",
    "print clf.predict(test_data_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Остальные классификаторы были отброшены из -за меньшей точности, несмотря на то, что некоторые из них (например LR) обучаются заметно быстрее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохраним классификатор и векторизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "# save the classifier\n",
    "with open('my_dumped_classifier.pkl', 'wb') as fid:\n",
    "    cPickle.dump(clf, fid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vectorizer.pkl', 'wb') as fid:\n",
    "    cPickle.dump(vectorizer,fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['Review']=[i[0] for i in test]\n",
    "df['Sentiment']=[int(i[1]) for i in test]\n",
    "df.to_csv('testRev.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ужасно\n"
     ]
    }
   ],
   "source": [
    "print df['Review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
